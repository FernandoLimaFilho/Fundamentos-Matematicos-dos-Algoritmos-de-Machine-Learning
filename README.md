# Fundamentos MatemÃ¡ticos dos Algoritmos de Machine Learning

Este repositÃ³rio tem como objetivo apresentar os fundamentos matemÃ¡ticos por trÃ¡s dos principais algoritmos de Machine Learning. A proposta Ã© oferecer uma abordagem didÃ¡tica e aprofundada, conectando teoria e prÃ¡tica por meio de conteÃºdos matemÃ¡ticos e implementaÃ§Ãµes em Python.

## ğŸ” Objetivos

- Explorar os conceitos fundamentais de Ãlgebra Linear, CÃ¡lculo, EstatÃ­stica e OtimizaÃ§Ã£o aplicados ao aprendizado de mÃ¡quina.
- Apresentar a deduÃ§Ã£o matemÃ¡tica e a lÃ³gica por trÃ¡s de algoritmos clÃ¡ssicos.
- Disponibilizar notebooks interativos com explicaÃ§Ãµes passo a passo e cÃ³digos funcionais.
- Servir como base para estudos autodidatas, cursos, workshops ou disciplinas na Ã¡rea de IA.

---

## âœ… Algoritmos abordados (em desenvolvimento)

- [x] RegressÃ£o Linear
- [ ] RegressÃ£o LogÃ­stica
- [ ] K-Means
- [ ] Support Vector Machines (SVM)
- [ ] Redes Neurais
- [ ] Ãrvores de DecisÃ£o
- [ ] PCA (AnÃ¡lise de Componentes Principais)

---

## ğŸ“˜ 1. RegressÃ£o Linear

### ğŸ’¡ IntuiÃ§Ã£o

A regressÃ£o linear Ã© um modelo supervisionado utilizado para prever uma variÃ¡vel contÃ­nua a partir de uma ou mais variÃ¡veis independentes. A ideia central Ã© ajustar uma reta (ou hiperplano) que melhor representa a tendÃªncia dos dados.

### ğŸ“ FormulaÃ§Ã£o MatemÃ¡tica

O modelo Ã© dado por:

\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
\]

ou, em notaÃ§Ã£o vetorial:

\[
\hat{y} = \mathbf{X}\boldsymbol{\beta}
\]

Onde:
- \(\mathbf{X}\) Ã© a matriz de entrada (com uma coluna de 1s para o termo de bias),
- \(\boldsymbol{\beta}\) Ã© o vetor de coeficientes a serem aprendidos.

### ğŸ¯ FunÃ§Ã£o de Custo

A funÃ§Ã£o de custo usada Ã© o **Erro QuadrÃ¡tico MÃ©dio (MSE)**:

\[
J(\boldsymbol{\beta}) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
\]

O objetivo do algoritmo Ã© minimizar essa funÃ§Ã£o.

### âš™ï¸ SoluÃ§Ã£o AnalÃ­tica

A minimizaÃ§Ã£o da funÃ§Ã£o de custo pode ser feita de forma fechada por:

\[
\boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\]

### ğŸ’» ImplementaÃ§Ã£o em Python

VocÃª pode conferir a implementaÃ§Ã£o detalhada em [notebooks/regressao_linear.ipynb](notebooks/regressao_linear.ipynb), onde apresentamos:
- GeraÃ§Ã£o de dados sintÃ©ticos
- Ajuste manual dos parÃ¢metros
- SoluÃ§Ã£o com equaÃ§Ã£o normal
- ComparaÃ§Ã£o com `scikit-learn`

---

## ğŸ“ Estrutura do RepositÃ³rio

